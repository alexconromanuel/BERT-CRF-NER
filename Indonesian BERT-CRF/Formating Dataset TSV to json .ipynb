{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40b12c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json, codecs\n",
    "\n",
    "import pandas as pd\n",
    "from itertools import groupby\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dc6b577",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"SINGGALANGv2.tsv\"\n",
    "df_raw = pd.read_csv(file_path,\n",
    "                     sep=\"\\t\",\n",
    "                     names=[\"token\", \"ne\"],\n",
    "                     skip_blank_lines=False,\n",
    "                     quoting=csv.QUOTE_NONE,\n",
    "                     encoding=\"utf-8\")\n",
    "# df_raw = df_raw.loc[:39]\n",
    "# df_raw = df_raw.loc[40:100]\n",
    "# df_raw = df_raw.loc[19:65]\n",
    "# df_raw = df_raw.loc[83:100]\n",
    "# df_raw = df_raw.loc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bd31d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentence = []\n",
    "list_entity = []\n",
    "\n",
    "list_tmp = []\n",
    "\n",
    "for row in df_raw.itertuples():\n",
    "    \n",
    "    if pd.isna(row.token) != True and pd.isna(row.ne) != True:\n",
    "        list_tmp.append(tuple((row.token, row.ne)))\n",
    "    else:\n",
    "        list_sentence.append([e[0] for e in list_tmp])\n",
    "        list_entity.append([e[1] for e in list_tmp])\n",
    "        list_tmp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7b4182b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48981it [00:00, 61302.93it/s]\n"
     ]
    }
   ],
   "source": [
    "def sequences_chunking(list_token, list_ne):\n",
    "    \"\"\"\n",
    "    Kelompokkan sekuens token dengan named entity yang bertetangga.\n",
    "    Simpan sekuens yang merupakan named entity\n",
    "    \"\"\"\n",
    "    list_tmp = []\n",
    "    list_res = []\n",
    "    curr_offset = 0\n",
    "    for ix, ex in enumerate(list_ne):\n",
    "        tkn = list_token[ix]\n",
    "        if ix == 0:\n",
    "            if ex != \"O\": list_tmp.append([curr_offset, tkn, ex])\n",
    "            curr_offset += len(list_token[ix]) + 1\n",
    "        elif ix < len(list_ne)-1:\n",
    "            if ex == list_ne[ix-1]:\n",
    "                if ex != \"O\": list_tmp.append([curr_offset, tkn, ex])\n",
    "            else:\n",
    "                list_res.append(list_tmp)\n",
    "                list_tmp = []\n",
    "                if ex != \"O\": list_tmp.append([curr_offset, tkn, ex])\n",
    "            curr_offset += len(list_token[ix]) + 1\n",
    "        else:\n",
    "            if ex != \"O\": list_tmp.append([curr_offset, tkn, ex])\n",
    "            list_res.append(list_tmp)\n",
    "            list_tmp = []\n",
    "    list_res = [i for i in list_res if i != []]\n",
    "    return list_res\n",
    "\n",
    "def entities_extraction(list_token, list_ne):\n",
    "    \"\"\"\n",
    "    Ekstraksi informasi entitas.\n",
    "    Simpan daftarnya untuk setiap kalimat\n",
    "    \"\"\"\n",
    "    list_res = []\n",
    "    for ex in sequences_chunking(list_token, list_ne):\n",
    "        text = \" \".join([ey[1] for ey in ex])\n",
    "        label = [ey[2] for ey in ex][0]\n",
    "        start_offset = [ey[0] for ey in ex][0]\n",
    "        end_offset = start_offset + len(text)\n",
    "        list_res.append([text, label, start_offset, end_offset])\n",
    "    return list_res\n",
    "\n",
    "list_singgalang = []\n",
    "ne_id = 0\n",
    "for ix, ex in tqdm(enumerate(list_sentence)):\n",
    "    list_ne_info = entities_extraction(list_sentence[ix], list_entity[ix])\n",
    "    list_ents = []\n",
    "    for ey in list_ne_info:\n",
    "        list_ents.append({\n",
    "            \"entity_id\": ne_id,\n",
    "            \"text\": ey[0],\n",
    "            \"label\": ey[1],\n",
    "            \"start_offset\": ey[2],\n",
    "            \"end_offset\": ey[3]\n",
    "        })\n",
    "        ne_id+=1\n",
    "    \n",
    "    dict_data = {\n",
    "        \"doc_id\": \"Singgalang#\" + str(ix),\n",
    "        \"doc_text\": \" \".join(ex),\n",
    "        \"entities\": list_ents\n",
    "    }\n",
    "    list_singgalang.append(dict_data)\n",
    "\n",
    "with open(\"singgalang.json\", \"wb\") as f:\n",
    "    json.dump(list_singgalang,\n",
    "              codecs.getwriter(\"utf-8\")(f),\n",
    "              ensure_ascii=False,\n",
    "              indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5632b674",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('3.9.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "0d496e974078952a239ae335f194ea837603150eb269773ca053418e7cd0d49b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
